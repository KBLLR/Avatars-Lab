# Avatar Labs

A Vite-based house that validates TalkingHead + HeadAudio integration against local MLX services (LLM + Audio). It ships deterministic fixtures, contract tests, browser harnesses, and Playwright E2E checks without relying on Ready Player Me or OpenAI cloud.

## What It Validates
- MLX LLM + Audio health and model listing
- MLX TTS produces decodable WAV audio
- Optional STT roundtrip from MLX Audio
- Data-pool event logs for audio generation/transcription
- HeadAudio viseme callbacks in a browser harness
- TalkingHead streaming playback with a local GLB avatar
- MLX-backed conversation loop (mic -> STT -> LLM -> TTS -> TalkingHead)
- Stage mode for uploaded songs with MLX STT transcript + HeadAudio visemes

## Requirements
- Node 20+
- MLX services running locally (see `services.manifest.json`)
- Playwright browsers installed: `npx playwright install`

## One-Command Runs
```bash
zsh houses/avatar-labs/scripts/smoke.zsh
zsh houses/avatar-labs/scripts/e2e.zsh
zsh houses/avatar-labs/scripts/all.zsh
```

## Dev Server (Auto-Start MLX Services)
```bash
cd /Users/davidcaballero/core-x-kbllr_0/houses/avatar-labs
npm run dev
```
This will:
- generate `.env.ecosystem` if missing
- write `.env.local` from the registry/manifest
- write `public/models/registry.json` for Settings page
- start MLX LLM + Audio services (if not already running)
- launch Vite

To run only the UI: `npm run dev:ui`

## MLX Conversation Lab
Open `mlx-conversation.html` from the Vite dev server for a local, MLX-backed version of the HeadAudio OpenAI demo. It supports push-to-talk and auto-VAD modes, plus TalkingHead function calls for mood, gestures, and emojis.

Avatar choices are loaded from `public/avatars/manifest.json`. Add `.glb` files under `public/avatars` and update the manifest to expose them in the UI.

## Stage (Music Video)
Open `stage.html` to upload songs, generate a transcript with MLX STT, and perform with HeadAudio-driven visemes plus animated lighting and gesture cues.

## Info + Settings
- `info.html` describes the upstream repos and the lab purpose.
- `settings.html` shows the resolved MLX endpoints, model registry snapshot, and HeadTTS options.

## Configuration
The runner scripts resolve endpoints and models from `services.manifest.json` and `model-zoo/registry.json`, with env overrides:

- `MLX_REGISTRY_PATH`
- `MLX_LLM_BASE_URL`
- `MLX_AUDIO_BASE_URL`
- `MLX_DEFAULT_LLM_MODEL`
- `MLX_DEFAULT_TTS_MODEL`
- `MLX_DEFAULT_STT_MODEL`
- `MLX_DEFAULT_TTS_VOICE`

The browser harnesses receive values via `VITE_MLX_AUDIO_BASE_URL`, `VITE_MLX_DEFAULT_TTS_MODEL`, and `VITE_MLX_DEFAULT_TTS_VOICE` (set automatically by scripts).

## HeadTTS Bridge (Optional)
A minimal HeadTTS-compatible REST bridge is provided for local demos:

```bash
node houses/avatar-labs/packages/headtts-bridge/server.mjs
```

Defaults to `http://127.0.0.1:6677` and accepts `POST /v1/synthesize` with `input`, `voice`, `language`, `speed`, `audioEncoding`.

## Fixtures
- `fixtures/avatars/brunette.glb` (TalkingHead sample avatar)
- `fixtures/audio/hello.wav` (deterministic speech clip)
- `fixtures/prompts/tts_prompt.txt`

## Results
Tests write artifacts under `houses/avatar-labs/results/` (ignored in git).

## Known Limitations
- Word/viseme timings are approximate when generated by the bridge.
- E2E tests require WebGL; they will skip if unavailable.

## Security
This house hard-fails if `OPENAI_API_KEY` is set to avoid accidental outbound calls.
