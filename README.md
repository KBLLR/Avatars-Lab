# Avatar Labs

A Vite-based house that validates TalkingHead + HeadAudio integration against local MLX services (LLM + Audio). It ships deterministic fixtures, contract tests, browser harnesses, and Playwright E2E checks without relying on Ready Player Me or OpenAI cloud.

## Entry Points
| Page | Purpose |
| --- | --- |
| `stage.html` | Main performance stage (legacy scheduler, production default) |
| `engine-lab.html` | Multi-layer performance engine with timeline editor |
| `dance-studio.html` | Animation/choreography authoring |
| `gestures-lab.html` | Gesture and blendshape authoring |
| `multi-modal.html` | VLM + voice conversation |
| `mlx-conversation.html` | Push-to-talk / VAD chat loop with TalkingHead |
| `info.html` | System info display |
| `settings.html` | Configuration panel |

## Technical State (2025-12-28)
- **Performance engines**: `stage.html` uses the legacy action scheduler; `engine-lab.html` opts into the new timeline engine.
- **Timeline layers**: `viseme`, `dance`, `blendshape`, `emoji`, `lighting`, `camera`, `fx` blocks with per-layer property editors.
- **Subtitles**: new renderer in `src/ui/subtitles.ts` with multiple presets (karaoke/cinema/minimal/neon/retro).
- **Avatar metadata**: `public/avatars/manifest.json` now supports structured entries (body, voice_id, intro/background, project, tags, default_mood). Loaders still accept the legacy string list.
- **Voices**: `voice_id` is used to preselect Kokoro voices in stage + multi-modal + mlx-conversation.
- **MLX runtime**: stage UI includes load/unload/status controls via `/internal/models/*` endpoints.

## What It Validates
- MLX LLM + Audio health and model listing
- MLX TTS produces decodable WAV audio
- Optional STT roundtrip from MLX Audio
- Data-pool event logs for audio generation/transcription
- HeadAudio viseme callbacks in a browser harness
- TalkingHead streaming playback with a local GLB avatar
- MLX-backed conversation loop (mic -> STT -> LLM -> TTS -> TalkingHead)
- Stage mode for uploaded songs with MLX STT transcript + HeadAudio visemes

## Requirements
- Node 20+
- MLX services running locally (see `services.manifest.json`)
- Playwright browsers installed: `npx playwright install`

## One-Command Runs
```bash
zsh houses/avatar-labs/scripts/smoke.zsh
zsh houses/avatar-labs/scripts/e2e.zsh
zsh houses/avatar-labs/scripts/all.zsh
```

## Dev Server (Auto-Start MLX Services)
```bash
cd /Users/davidcaballero/core-x-kbllr_0/houses/avatar-labs
npm run dev
```
This will:
- generate `.env.ecosystem` if missing
- write `.env.local` from the registry/manifest
- write `public/models/registry.json` for Settings page
- start MLX LLM + Audio services (if not already running)
- launch Vite

To run only the UI: `npm run dev:ui`

## MLX Conversation Lab
Open `mlx-conversation.html` from the Vite dev server for a local, MLX-backed version of the HeadAudio OpenAI demo. It supports push-to-talk and auto-VAD modes, plus TalkingHead function calls for mood, gestures, and emojis.

Avatar choices are loaded from `public/avatars/manifest.json`. Add `.glb` files under `public/avatars` and update the manifest to expose them in the UI (supports metadata like body and voice_id).

## Stage (Music Video)
Open `stage.html` to upload songs, generate a transcript with MLX STT, and perform with HeadAudio-driven visemes plus animated lighting and gesture cues.

## Info + Settings
- `info.html` describes the upstream repos and the lab purpose.
- `settings.html` shows the resolved MLX endpoints, model registry snapshot, and HeadTTS options.

## Configuration
The runner scripts resolve endpoints and models from `services.manifest.json` and `model-zoo/registry.json`, with env overrides:

- `MLX_REGISTRY_PATH`
- `MLX_LLM_BASE_URL`
- `MLX_AUDIO_BASE_URL`
- `MLX_DEFAULT_LLM_MODEL`
- `MLX_DEFAULT_TTS_MODEL`
- `MLX_DEFAULT_STT_MODEL`
- `MLX_DEFAULT_TTS_VOICE`

The browser harnesses receive values via `VITE_MLX_AUDIO_BASE_URL`, `VITE_MLX_DEFAULT_TTS_MODEL`, and `VITE_MLX_DEFAULT_TTS_VOICE` (set automatically by scripts).

## HeadTTS Bridge (Optional)
A minimal HeadTTS-compatible REST bridge is provided for local demos:

```bash
node houses/avatar-labs/packages/headtts-bridge/server.mjs
```

Defaults to `http://127.0.0.1:6677` and accepts `POST /v1/synthesize` with `input`, `voice`, `language`, `speed`, `audioEncoding`.

## Fixtures
- `fixtures/avatars/brunette.glb` (TalkingHead sample avatar)
- `fixtures/audio/hello.wav` (deterministic speech clip)
- `fixtures/prompts/tts_prompt.txt`

## Results
Tests write artifacts under `houses/avatar-labs/results/` (ignored in git).

## Known Limitations
- Word/viseme timings are approximate when generated by the bridge.
- E2E tests require WebGL; they will skip if unavailable.
- `src/lab/event-bus.ts` and `src/talkinghead-e2e.ts` have pre-existing type errors.
- `src/stage.ts` is a large monolith and needs decomposition.

## Build Notes
- Recent builds warn about large chunks (500 kB+). Candidates include TalkingHead and audio decoder bundles.
- Consider code splitting via dynamic import in heavy entry points or `build.rollupOptions.output.manualChunks` in `vite.config.mjs`.

## Security
This house hard-fails if `OPENAI_API_KEY` is set to avoid accidental outbound calls.
